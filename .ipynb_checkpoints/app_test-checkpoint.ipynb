{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "import requests\n",
    "from flask import jsonify\n",
    "\n",
    "import os\n",
    "import json\n",
    "from ast import literal_eval\n",
    "import traceback\n",
    "\n",
    "application = Flask(__name__)\n",
    "\n",
    "\n",
    "#загружаем модели из файла\n",
    "vec = pickle.load(open(\"./models/tfidf.pickle\", \"rb\"))\n",
    "with open(\"./models/mlp_model.pkl\", 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "\n",
    "# тестовый вывод\n",
    "@application.route(\"/\")  \n",
    "def hello():\n",
    "    resp = {'message':\"Hello World!\"}\n",
    "    \n",
    "    response = jsonify(resp)\n",
    "    \n",
    "    return response\n",
    "\n",
    "def processing_message(message):\n",
    "    true_words = []\n",
    "    words = re.split(' ', message)\n",
    "    for word in words:\n",
    "        m = re.search('(\\w+)', word)\n",
    "        if m is not None:\n",
    "            good_word = m.group(0)\n",
    "            true_words.append(good_word)\n",
    "    stemming = PorterStemmer()\n",
    "    stemmed_message_list = [stemming.stem(word) for word in true_words]\n",
    "    stemmed_message = ' '.join(stemmed_message_list)\n",
    "    return stemmed_message_list\n",
    "\n",
    "\n",
    "# предикт категории\n",
    "#{\"user_message\":\"example123rfssg gsfgfd\"}\n",
    "@application.route(\"/categoryPrediction\" , methods=['GET', 'POST'])  \n",
    "def registration():\n",
    "    resp = {'message':'ok'\n",
    "           ,'category': -1\n",
    "           }\n",
    "\n",
    "    try:\n",
    "        getData = request.get_data()\n",
    "        json_params = json.loads(getData) \n",
    "        \n",
    "        #напишите прогноз и верните его в ответе в параметре 'prediction'\n",
    "        category = model.predict_proba(vec.transform([json_params['user_message']]).toarray()).tolist()\n",
    "        resp['category'] = category\n",
    "\n",
    "\n",
    "        \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        resp['message'] = e     \n",
    "    response = jsonify(resp)\n",
    "    \n",
    "    return response\n",
    "\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    port = int(os.getenv('PORT', 5000))\n",
    "    application.run(debug=False, port=port, host='0.0.0.0' , threaded=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример сохранения моделей и прогноза"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aquila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aquila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sumy.utils import get_stop_words as gsw1\n",
    "from stop_words import get_stop_words as gsw2\n",
    "\n",
    "import pickle\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['visitor', 'i', 'am', 'verifi', 'card', 'the', 'payment', 'all', 'charg', 'post', 'bank', 'sofia', 'hello', 'sofia', 'pleas', 'stand', 'onlin', 'take', 'some', 'time', 'resolv', 'the', 'issu', 'ill', 'an', 'updat', 'a', 'minut', 'thank', 'patienc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.9364379186007357, 0.06356207710931464, 4.289949842819412e-09]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#загружаем модели из файла\n",
    "vec = pickle.load(open(\"./models/tfidf.pickle\", \"rb\"))\n",
    "with open(\"./models/mlp_model.pkl\", 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "    \n",
    "def cleaning_message(text):\n",
    "    text = re.sub('\\[.*\\]', '', text)\n",
    "    text = re.sub(\"\\!\", '', text)\n",
    "    text = re.sub(\"\\'\", '', text)\n",
    "    text = re.sub(\"[^A-Za-z0-9^,!.\\/'+-=]\", ' ', text)\n",
    "    text = re.sub(\"\\s+\", ' ', text)    \n",
    "    return text\n",
    "\n",
    "def get_original_form(text):        \n",
    "    try:\n",
    "        words = re.split(' ', text)\n",
    "        true_words = []\n",
    "        for word in words:\n",
    "            m = re.search('(\\w+)',word)\n",
    "            if m is not None:\n",
    "                good_word = m.group(0)\n",
    "                true_words.append(good_word)\n",
    "        tagged = nltk.pos_tag(true_words) \n",
    "        tags = []\n",
    "        for tag in tagged:        \n",
    "            tags.append(tag[1])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(true_words)\n",
    "    stopWords = nltk.corpus.stopwords.words()\n",
    "    LANGUAGE = 'english'\n",
    "\n",
    "    sw0 = [\"yeah\",\"zola\",\"don\"]\n",
    "    sw1 = gsw1(LANGUAGE)\n",
    "    sw2 = gsw2('en')\n",
    "\n",
    "    sw0.extend(list(sw1))\n",
    "    sw0.extend(list(sw2))\n",
    "    \n",
    "    new_s = ''\n",
    "    new_lw = []\n",
    "    new_lt = []\n",
    "    new_lwt = []\n",
    "    for w,t,lw in zip(true_words, tags, tagged):\n",
    "        if t in ['NN','VB','DT','NNS','VBP','VB']:\n",
    "            new_s += w + ' '\n",
    "            new_lw.append(w)\n",
    "            new_lt.append(t)\n",
    "            new_lwt.append(lw)\n",
    "        elif w in sw0 or re.match('\\d+',w) is not None:\n",
    "            continue           \n",
    "    new_list = re.split(r'\\W+', new_s)\n",
    "    stemming = PorterStemmer()\n",
    "    stemmed_list = [stemming.stem(word) for word in new_list] \n",
    "    original_form = ' '.join(stemmed_list)\n",
    "    return original_form      \n",
    "\n",
    "\n",
    "\n",
    "#test predict\n",
    "message = \"visitor i am verifi card the payment all charg post bank sofia hello sofia pleas stand onlin take some time resolv the issu ill an updat a minut thank patienc\"\n",
    "message = message.lower()\n",
    "message = cleaning_message(message)\n",
    "message = get_original_form(message)\n",
    "model.predict_proba(vec.transform([message]).toarray()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visitor i am verifi card the payment all charg post bank sofia hello sofia pleas stand onlin take some time resolv the issu ill an updat a minut thank patienc \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.8976279624223987, 0.10237203592529492, 1.6523064078978437e-09]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"\\nChat transcript:\\nVisitor: I am attempting to verify my card however the payment is not posting. All charges typically post immediately with my bank.\\nSofia: Hello!\\nSofia: Please stand by online, as it may take some time to resolve the issue. I'll provide you with an update in a few minutes. Thank you for your patience.\\n[Visitor page reloaded. New URL: https://secure.xsolla.com/paystation3/?access_token=vlp0QhPAE54cVIsykIWwTE0BZp70mAQy ]\\n[Visitor page reloaded. New URL: https://secure.xsolla.com/paystation3/?access_token=vlp0QhPAE54cVIsykIWwTE0BZp70mAQy ]\\n[Visitor page reloaded. New URL: https://secure.xsolla.com/paystation3/?access_token=vlp0QhPAE54cVIsykIWwTE0BZp70mAQy ]\"\n",
    "message = message.lower()\n",
    "message = cleaning_message(message)\n",
    "message = get_original_form(message)\n",
    "print(message)\n",
    "model.predict_proba(vec.transform([message]).toarray()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
